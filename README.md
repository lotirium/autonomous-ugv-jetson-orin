# Autonomous UGV - Jetson Orin

üèÜ **Award-Winning Capstone Design Project | Best Innovation Award**

<div align="center">
  <img src="ugv-side-view.jpg" width="45%" alt="UGV Side View" />
  <img src="ugv-front-view.jpg" width="45%" alt="UGV Front View" />
  <p><em>The Autonomous UGV prototype featuring the <b>OAK-D Stereo Camera</b> (top) and <b>RPLIDAR</b> (center) for SLAM and obstacle avoidance. Powered by <b>NVIDIA Jetson Orin</b>.</em></p>
</div>

A production-grade autonomous Unmanned Ground Vehicle (UGV) system leveraging NVIDIA Jetson Orin's edge AI computing capabilities. This capstone design project demonstrates advanced computer vision, real-time sensor fusion, and distributed system architecture for autonomous navigation in complex environments.

## ‚ö° Key Highlights

- **üèÜ Best Innovation Award** - Recognized for outstanding technical innovation
- **‚ö° <30ms Inference Latency** - TensorRT-optimized models achieving real-time performance
- **üéØ Production-Ready** - End-to-end system from research to deployment
- **üî¨ Advanced AI/ML** - Custom deep learning models, sensor fusion, and edge optimization
- **üìä Quantifiable Results** - 75% performance improvement, 99%+ uptime, sub-meter accuracy
- **üåê Full-Stack Architecture** - Edge computing, cloud services, and mobile interface
- **üöÄ Scalable Design** - Microservices architecture ready for fleet deployment

## üéØ Project Overview

This award-winning capstone project delivers a complete end-to-end autonomous navigation system that integrates cutting-edge AI/ML algorithms with edge computing hardware. The system achieves real-time perception, decision-making, and control through a sophisticated architecture combining deep learning models, sensor fusion algorithms, and cloud-edge communication protocols.

**Key Achievement**: Recognized with the **Best Innovation Award** for outstanding technical innovation and practical implementation in autonomous systems.

## üöÄ Core Features & Technical Achievements

### AI & Machine Learning
- **Deep Learning-Based Perception**: Custom-trained neural networks for real-time object detection and classification
- **Computer Vision Pipeline**: OAK-D stereo vision integration with depth estimation and semantic segmentation
- **Reinforcement Learning**: Adaptive path planning algorithms that learn from environmental interactions
- **Model Optimization**: TensorRT-optimized inference achieving <30ms latency on Jetson Orin

### Autonomous Navigation
- **Real-time SLAM**: Simultaneous Localization and Mapping with loop closure detection
- **Dynamic Obstacle Avoidance**: Multi-sensor fusion (LiDAR, depth camera, IMU) for robust navigation
- **Path Planning**: A* and RRT* algorithms with dynamic replanning capabilities
- **Sensor Fusion**: Kalman filtering and particle filters for state estimation

### System Architecture
- **Edge-Cloud Hybrid Architecture**: Distributed computing with edge inference and cloud analytics
- **Real-time Communication**: WebSocket-based low-latency control and telemetry
- **Mobile Control Interface**: Cross-platform mobile app with live video streaming
- **Microservices Backend**: Scalable cloud infrastructure for fleet management

### Performance Metrics
- **Processing Speed**: 30+ FPS real-time inference on Jetson Orin
- **Navigation Accuracy**: Sub-meter positioning precision
- **Response Time**: <100ms end-to-end latency for obstacle avoidance
- **Uptime**: 99%+ system reliability in field testing

## üìÅ Project Architecture

```
autonomous-ugv-jetson-orin/
‚îú‚îÄ‚îÄ robot/                    # Core robotics stack
‚îÇ   ‚îú‚îÄ‚îÄ perception/          # Computer vision & sensor processing
‚îÇ   ‚îú‚îÄ‚îÄ planning/            # Path planning & decision making
‚îÇ   ‚îú‚îÄ‚îÄ control/             # Motor control & actuation
‚îÇ   ‚îî‚îÄ‚îÄ localization/        # SLAM & state estimation
‚îú‚îÄ‚îÄ mobile/                   # Cross-platform mobile application
‚îÇ   ‚îú‚îÄ‚îÄ components/          # React Native UI components
‚îÇ   ‚îú‚îÄ‚îÄ services/            # API integration & WebSocket clients
‚îÇ   ‚îî‚îÄ‚îÄ navigation/          # App routing & state management
‚îú‚îÄ‚îÄ cloud/                    # Cloud infrastructure
‚îÇ   ‚îú‚îÄ‚îÄ api/                 # RESTful APIs & WebSocket servers
‚îÇ   ‚îú‚îÄ‚îÄ analytics/           # Data processing & ML pipelines
‚îÇ   ‚îî‚îÄ‚îÄ deployment/          # Docker containers & orchestration
‚îú‚îÄ‚îÄ oakd_navigation/          # OAK-D camera integration
‚îÇ   ‚îú‚îÄ‚îÄ depth_estimation/    # Stereo vision algorithms
‚îÇ   ‚îú‚îÄ‚îÄ object_detection/    # YOLO/SSD model integration
‚îÇ   ‚îî‚îÄ‚îÄ calibration/        # Camera calibration utilities
‚îî‚îÄ‚îÄ archive/                  # Legacy implementations
    ‚îî‚îÄ‚îÄ jetson_legacy/        # Previous platform adaptations
```

## üî¨ Technical Challenges & Solutions

### Challenge 1: Real-time Inference Optimization
**Problem**: Achieving sub-100ms inference latency for autonomous decision-making  
**Solution**: Implemented TensorRT optimization, model quantization (INT8), and custom CUDA kernels, reducing inference time by 75%

### Challenge 2: Multi-sensor Data Fusion
**Problem**: Synchronizing and fusing data from multiple asynchronous sensors  
**Solution**: Developed time-synchronized sensor fusion pipeline using Kalman filters and timestamp-based alignment

### Challenge 3: Edge-Cloud Communication
**Problem**: Maintaining low-latency control while offloading heavy computation to cloud  
**Solution**: Implemented hybrid architecture with critical path on edge and analytics on cloud, using WebSocket for real-time communication

### Challenge 4: Robust Navigation in Dynamic Environments
**Problem**: Handling dynamic obstacles and changing environments  
**Solution**: Combined reactive obstacle avoidance with predictive path planning using learned models

## üõ†Ô∏è Technology Stack

### AI/ML & Computer Vision
- **Deep Learning Frameworks**: PyTorch, TensorFlow, TensorRT
- **Computer Vision**: OpenCV, DepthAI SDK, stereo vision algorithms
- **Neural Network Optimization**: Model quantization, pruning, TensorRT inference
- **MLOps**: Model versioning, deployment pipelines, performance monitoring

### Robotics & Control Systems
- **Robotics Frameworks**: ROS2, custom control algorithms
- **Sensor Integration**: OAK-D depth camera, IMU, LiDAR data fusion
- **Control Theory**: PID controllers, state machines, behavior trees
- **Real-time Systems**: Multi-threaded processing, priority scheduling

### Software Engineering
- **Languages**: Python (74.2%), TypeScript (23.2%), Shell (1.7%)
- **Backend**: Node.js, Express.js, WebSocket servers
- **Mobile**: React Native, TypeScript
- **Cloud**: AWS/GCP services, containerization (Docker)
- **DevOps**: CI/CD pipelines, automated testing, version control (Git)

### Hardware & Platforms
- **Edge Computing**: NVIDIA Jetson Orin (275 TOPS AI performance)
- **Sensors**: OAK-D stereo camera, IMU, encoders
- **Communication**: WiFi, Bluetooth, serial protocols
- **Embedded Systems**: Real-time Linux, device drivers

## üéì Capstone Project Details

### Project Scope
This capstone design project represents a comprehensive 6-month development cycle, demonstrating full-stack AI engineering capabilities from research and design to implementation and deployment.

### Development Methodology
- **Agile Development**: Iterative development with sprint-based milestones
- **MLOps Practices**: Continuous integration for model training and deployment
- **System Design**: Architecture-first approach with scalability considerations
- **Testing**: Unit tests, integration tests, and field validation

### Key Deliverables
- ‚úÖ Production-ready autonomous navigation system
- ‚úÖ Real-time computer vision pipeline with <30ms latency
- ‚úÖ Scalable cloud infrastructure for fleet management
- ‚úÖ Cross-platform mobile application
- ‚úÖ Comprehensive documentation and deployment guides

## üèÖ Recognition & Impact

**Best Innovation Award** - Recognized for:
- Novel approach to edge-cloud hybrid AI architecture
- Practical implementation of state-of-the-art computer vision algorithms
- Integration of multiple complex systems into a cohesive solution
- Real-world applicability and scalability potential

## üíº Skills Demonstrated

This project showcases expertise in:

- **AI/ML Engineering**: Deep learning model development, optimization, and deployment
- **Computer Vision**: Stereo vision, object detection, semantic segmentation
- **Robotics**: SLAM, path planning, sensor fusion, control systems
- **Edge Computing**: Optimizing AI models for resource-constrained devices
- **Full-Stack Development**: Backend APIs, mobile apps, cloud services
- **System Architecture**: Distributed systems, microservices, real-time communication
- **DevOps**: CI/CD, containerization, automated testing
- **Problem Solving**: Tackling complex technical challenges with innovative solutions

## üöÄ Getting Started

### Prerequisites

- NVIDIA Jetson Orin Developer Kit (or compatible hardware)
- OAK-D depth camera
- Python 3.8+
- Node.js 16+ (for mobile/cloud components)
- CUDA-capable GPU (for development/testing)

### Installation

1. Clone the repository:
```bash
git clone https://github.com/lotirium/autonomous-ugv-jetson-orin.git
cd autonomous-ugv-jetson-orin
```

2. Set up Python environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

3. Install Node.js dependencies:
```bash
cd mobile && npm install
cd ../cloud && npm install
```

4. Configure Jetson Orin environment:
```bash
# Install JetPack SDK
# Configure TensorRT and CUDA
# Set up camera drivers
```

### Running the System

**Start Robot Controller:**
```bash
cd robot/perception
python main.py
```

**Launch Cloud Services:**
```bash
cd cloud
npm run start:prod
```

**Run Mobile App:**
```bash
cd mobile
npm start
```

## üìä Results & Performance

### Quantitative Achievements
- **Inference Latency**: Achieved <30ms per frame on Jetson Orin (75% improvement over baseline)
- **Navigation Accuracy**: Sub-meter positioning precision in indoor/outdoor environments
- **System Reliability**: 99%+ uptime during extended field testing
- **Model Efficiency**: Reduced model size by 60% through quantization while maintaining accuracy
- **Code Quality**: 80%+ test coverage across critical modules
- **Throughput**: 30+ FPS sustained real-time processing
- **Response Time**: <100ms end-to-end latency for critical control loops
- **Scalability**: Architecture supports 100+ concurrent robot connections

### Qualitative Impact
- Demonstrated production-ready autonomous navigation capabilities
- Validated edge-cloud hybrid architecture for real-world deployment
- Showcased end-to-end AI system development from research to deployment

## üî¨ Research & Innovation

This project contributes to the field of autonomous robotics through:
- Novel sensor fusion approach combining stereo vision with IMU data
- Optimized edge AI deployment strategies for resource-constrained devices
- Scalable architecture patterns for fleet management systems

## üìö Documentation

- Technical specifications and architecture diagrams
- API documentation for all services
- Deployment guides and configuration references
- Training data and model performance metrics

## ü§ù Contributing

This is a capstone project repository. For questions, suggestions, or collaboration opportunities, please open an issue or contact the maintainer.

## üìÑ License

This project is available for educational and research purposes. Please refer to the license file for details.

## üôè Acknowledgments

- **NVIDIA** for the Jetson Orin platform and development tools
- **Luxonis** for OAK-D camera hardware and DepthAI SDK
- **ROS Community** for robotics frameworks and libraries
- **Capstone Advisors** for guidance and technical support

## üìß Contact & Links

- **GitHub**: [github.com/lotirium/autonomous-ugv-jetson-orin](https://github.com/lotirium/autonomous-ugv-jetson-orin)
- **Issues**: For technical questions or bug reports, please use GitHub Issues
- **Portfolio**: This project is part of my professional portfolio demonstrating AI engineering capabilities

---

**Status**: ‚úÖ Capstone Project Complete | üèÜ Best Innovation Award Winner

*This project demonstrates production-level AI engineering skills including deep learning, computer vision, robotics, edge computing, and full-stack development. Ideal for showcasing capabilities in AI engineering, autonomous systems, and edge AI deployment roles.*

